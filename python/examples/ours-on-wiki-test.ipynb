{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites:\n",
    "\n",
    "* python 3.6, we use the f-string notation ( [pep 498](https://www.python.org/dev/peps/pep-0498/) ). Alternatively you can modify the code to use python 3.5-compatible formatting\n",
    "\n",
    "* Best way to get python is via [Anaconda](https://www.anaconda.com/download/)\n",
    "\n",
    "* You need to install `tinydb`: `pip install tinydb`\n",
    "\n",
    "* You need to build our python wrappers: `make bind`\n",
    "\n",
    "* You need faiss python wrappers\n",
    "\n",
    "* You need to build our FastText fork: `cd fastText ; make `\n",
    "\n",
    "* You need to get dataset from [Extreme Classification Repo](http://manikvarma.org/downloads/XC/XMLRepository.html)\n",
    "\n",
    "* The downloaded dataset has to be renamed to `train.txt` and `test.txt` (or you can fix the loader code)\n",
    "\n",
    "* Data should be in `<root>/data`, or you can change the paths in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system-related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add these directories to PYTHONPATH so that we can import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./faiss/'))\n",
    "sys.path.append(os.path.abspath('./python/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our functions to read Extreme-Repository data format and to transform it to FastText format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from experiments.data import get_data\n",
    "from misc.utils import to_ft, load_sift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read exrepo format, transform it to FastText format (stored in `./data/LSHTC-FT/*.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y, words_mask, labels_mask = get_data('./data/LSHTC', 'train', min_words=3, min_labels=3)\n",
    "to_ft(X, Y, './data/LSHTC-FT/train.txt')\n",
    "\n",
    "X, Y, *_ = get_data('./data/LSHTC', 'test', words_mask=words_mask, labels_mask=labels_mask)\n",
    "to_ft(X, Y, './data/LSHTC-FT/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fasttext commands to be run. They will generate our MIPS dataset.\n",
    "\n",
    "What they do is they first train a FastText model, and then instead of calling `predict()`,\n",
    "we'll go through each example and write to disk three things:\n",
    "\n",
    "* correct answers\n",
    "\n",
    "* hidden vectors from the model (our `QUERIES`)\n",
    "\n",
    "* output-weight matrix (our `BASE` vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     7,
     22
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cmd(*args, **kwargs):\n",
    "    args = ' '.join(args)\n",
    "    opts = ' '.join(f'-{k} {v}' for k, v in kwargs.items())\n",
    "    cmd  = f'./fastText/fasttext {args} {opts}'\n",
    "    \n",
    "    return cmd.split()\n",
    "\n",
    "train_cmd = make_cmd('supervised', \n",
    "                     input         = './data/LSHTC-FT/train.txt',\n",
    "                     output        = './data/LSHTC-FT/model.ft',\n",
    "                     minCount      = 5,\n",
    "                     minCountLabel = 5,\n",
    "                     lr            = 0.1,\n",
    "                     lrUpdateRate  = 100,\n",
    "                     dim           = 256,\n",
    "                     ws            = 5,\n",
    "                     epoch         = 25,\n",
    "                     neg           = 25,\n",
    "                     loss          = 'ns',\n",
    "                     thread        = 8,\n",
    "                     saveOutput    = 1)\n",
    "\n",
    "generate_cmd = make_cmd('to-fvecs',\n",
    "                        './data/LSHTC-FT/model.ft.bin',\n",
    "                        './data/LSHTC-FT/test.txt',\n",
    "                        './data/LSHTC-FT/fvecs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs bash command prepared above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.call(train_cmd)\n",
    "subprocess.call(generate_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "We're gonna run some `Indices` on the data that we generated above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from tinydb import TinyDB, where\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./faiss/'))\n",
    "sys.path.append(os.path.abspath('./python/'))\n",
    "\n",
    "import faiss\n",
    "import mips\n",
    "\n",
    "from experiments.data import get_data\n",
    "from misc.utils import to_ft, load_sift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer():\n",
    "    \"\"\" Simple context manager to convieniently measure times between enter and exit \"\"\"\n",
    "    \n",
    "    class Clock:\n",
    "        elapsed = 0\n",
    "        \n",
    "    t0 = time.time()\n",
    "    \n",
    "    yield Clock\n",
    "    \n",
    "    Clock.elapsed = time.time() - t0\n",
    "\n",
    "\n",
    "def search(idx, data, k):\n",
    "    \"\"\" Search top-k items in `data` using `idx` \n",
    "    \n",
    "    This is needed because currently our wrappers return 1-D arrays, so we need to reshape them\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    D, I = idx.search(data, k)\n",
    "    D, I = D.reshape(-1, k), I.reshape(-1, k)\n",
    "    \n",
    "    return D, I\n",
    "\n",
    "\n",
    "def compute_p1(G, I):\n",
    "    \"\"\" Compute precision-at-1 for groundtruth `G` and predicted indices `I` \"\"\"\n",
    "    \n",
    "    p1 = 0.\n",
    "    for i, item in enumerate(I):\n",
    "        p1 += float(int(item) in G[i])\n",
    "\n",
    "    p1 /= len(G)\n",
    "    \n",
    "    return p1\n",
    "\n",
    "\n",
    "def test_idx(IdxClass, params, xb, xq, G, k=100):\n",
    "    \"\"\" Train and test the given Index class with given params \n",
    "    \n",
    "    Use the provided base, query, and groundtruth vectors. \n",
    "    \n",
    "    Index will predict top-`k` entries.\n",
    "    \n",
    "    In case of failure, the exception is returned as string\n",
    "    \n",
    "    This function returns a Report dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "\n",
    "        idx = IdxClass(**params)\n",
    "\n",
    "        with timer() as train_t:\n",
    "            idx.train(xb)\n",
    "            idx.add(xb)\n",
    "\n",
    "        with timer() as search_t:\n",
    "            _, I = search(idx, xq, k)\n",
    "\n",
    "        p1 = compute_p1(G, I[:, 0])\n",
    "\n",
    "        report = make_report(IdxClass, params, p1, train_t.elapsed, search_t.elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('FAILED: ' + str(e))\n",
    "        report = str(e)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def now():\n",
    "    \"\"\" Helper function to format current timestamp \"\"\"\n",
    "    \n",
    "    return datetime.datetime.fromtimestamp(time.time()).strftime(\"%d-%m-%y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def make_report(IdxClass, params, p1, train_t, search_t):\n",
    "    \"\"\" Create a Report dictionary for given set of parameters \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'ID': uuid.uuid4().hex,\n",
    "        'algo': IdxClass.__name__,\n",
    "        'params': params,\n",
    "        'p1': p1,\n",
    "        'train_t': train_t,\n",
    "        'search_t': search_t\n",
    "    }\n",
    "\n",
    "\n",
    "def add_result(r):\n",
    "    \"\"\" Add result `r` to the database \"\"\"\n",
    "    \n",
    "    if isinstance(r, dict):\n",
    "        algo, params, p1, t = r['algo'], r['params'], r['p1'], r['search_t']\n",
    "        rep = f'(params={params}, p1={p1:.2f}, t={t:.2f})'\n",
    "    else:\n",
    "        rep = r\n",
    "    \n",
    "    logger.info(f'Adding: {rep}')\n",
    "    \n",
    "    def result_adder(doc):\n",
    "        doc['results'].append(r)\n",
    "        \n",
    "    DB.update(result_adder, where('ID') == ID)\n",
    "\n",
    "\n",
    "def test(IdxClass, **params):\n",
    "    \"\"\" Even higher-level wrapper for testing \"\"\"\n",
    "    \n",
    "    return test_idx(IdxClass, params, xb, xq, G, k=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the database we'll store the results in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm ./data/results/ad-hoc-db.json\n",
    "DB = TinyDB('./data/results/ad-hoc-db.json')\n",
    "ID = uuid.uuid4().hex\n",
    "\n",
    "info = dict(\n",
    "    ID = ID,\n",
    "    name = 'ad-hoc-results',\n",
    "    date = now(),\n",
    "    results = []\n",
    ")\n",
    "\n",
    "DB.insert(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xq = load_sift('./data/LSHTC-FT/fvecs.hid.fvecs', dtype=np.float32)\n",
    "xb = load_sift('./data/LSHTC-FT/fvecs.wo.fvecs', dtype=np.float32)\n",
    "\n",
    "_n, d, c = xq.shape[0], xq.shape[1], xb.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not run on all queries, but on random subset of `lIMIT` queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIMIT = 250_000\n",
    "\n",
    "inds = np.random.choice(np.arange(_n), LIMIT, replace=False)\n",
    "xq   = xq[inds, :]\n",
    "\n",
    "xq = np.copy(np.ascontiguousarray(xq), order='C')\n",
    "xb = np.copy(np.ascontiguousarray(xb), order='C')\n",
    "\n",
    "n = xq.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = []\n",
    "for line in open('./data/LSHTC-FT/fvecs.labels.txt'):\n",
    "    G.append({int(y) for y in line.split()})\n",
    "G = [G[idx] for idx in inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loaded dataset of {_n:_}, {d:_}-dimensionsl queries (examples), but limiting to {LIMIT:_} queries\")\n",
    "logger.info(f\"The dataset contains {c:_} classes, and more than one class can be positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxies\n",
    "\n",
    "These are simple helper classes that will delegate method calls to underlying index, \n",
    "but expose a consistent API to create these Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IVFIndex:\n",
    "    def __init__(self, d, size, nprobe):\n",
    "        self.index = faiss.index_factory(d, f\"IVF{size},Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "        self.index.nprobe = nprobe\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.index, name)\n",
    "    \n",
    "    \n",
    "class KMeansIndex:\n",
    "    def __init__(self, d, layers, nprobe, m, U):\n",
    "        self.aug = mips.MipsAugmentationShrivastava(d, m, U)\n",
    "        self.index = mips.IndexHierarchicKmeans(d, layers, nprobe, self.aug, False)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.index, name)\n",
    "    \n",
    "        \n",
    "class FlatIndex:\n",
    "    def __init__(self, d):\n",
    "        self.index = faiss.IndexFlatIP(d)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.index, name)\n",
    "    \n",
    "class AlshIndex:\n",
    "    def __init__(self, d: int, L: int, K: int, r: int, m: int, U: int):\n",
    "        self.aug = mips.MipsAugmentationShrivastava(d, m, U)\n",
    "        self.index = mips.AlshIndex(d, L, K, r, self.aug)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.index, name)\n",
    "       \n",
    "class QuantIndex:\n",
    "    def __init__(self, dim: int, subspace_count: int, centroid_count: int):\n",
    "        self.index = mips.IndexSubspaceQuantization(dim, subspace_count, centroid_count)\n",
    "       \n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.index, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1\n",
    "\n",
    "Here we start calling our indices on the data, and log the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [4096]:\n",
    "    for nprobe in [1, 16, 32, 64, 128]:\n",
    "        add_result(\n",
    "            test(\n",
    "                IVFIndex, d=d, size=size, nprobe=nprobe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layers in [2]:\n",
    "    for nprobe in [1, 16, 32, 64, 128]:\n",
    "        add_result(\n",
    "            test(\n",
    "                KMeansIndex, d=d, layers=layers, nprobe=nprobe, m=5, U=0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [4096]:\n",
    "    for nprobe in [256, 512]:\n",
    "        add_result(\n",
    "            test(\n",
    "                IVFIndex, d=d, size=size, nprobe=nprobe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for layers in [2]:\n",
    "    for nprobe in [256, 512]:\n",
    "        add_result(\n",
    "            test(\n",
    "                KMeansIndex, d=d, layers=layers, nprobe=nprobe, m=5, U=0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in [3]:\n",
    "    for nprobe in [1, 16, 32, 64, 128]:\n",
    "        add_result(\n",
    "            test(\n",
    "                KMeansIndex, d=d, layers=layers, nprobe=nprobe, m=5, U=0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ALSH and quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for L in [2,4,8,16,32]:\n",
    "    for K in [2, 4, 8, 16, 32]:\n",
    "        add_result(\n",
    "            test(\n",
    "                AlshIndex, d=d, L=L, K=K, m=5, r=10, U=0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subspace in [2,4,8]:\n",
    "    for centroid in [2, 4, 8, 16, 32]:\n",
    "        add_result(\n",
    "            test(\n",
    "                QuantIndex, dim=d, subspace_count=subspace, centroid_count=centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "This is a flat index, i.e. full scan over queries, but implemented in an efficient way (`FAISS`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result(\n",
    "    test(\n",
    "        FlatIndex, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "DB.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
