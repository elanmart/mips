
\subsection{Conducting the experiments}

    We've conducted extensive experiments to evaluate performance of our algorithms. We've used
    four different datasets, described in detail in section~\ref{sec:datasets}, and compared
    our implementation with a highly-optimized, open source \texttt{FAISS} library released by
    \texttt{Facebook AI Research} group.

    \subsubsection{Evaluation metrics}

        We have compared the algorithms according to two metrics -- \texttt{test-time}
        and \texttt{precision@}$k$.

        \texttt{Test-time} is defined simply as a number of seconds it takes the index to return
        top-$100$ (approximate) nearest neighbours for each vector in a set of queries provided by the user.
        We assume that queries are available as a contiguous chunk of data kept in-memory.

        \texttt{Precision-@}$k$ can be intuitively described as a
        \textit{"number of relevant items in the list of $k$ returned items, divided by $k$".}

        More formally, given a vector of scores assigned to each item $\hat{\mathbf y} \in {\cal{R}}^{L}$, and a
        binary vector indicating which items are relevant $\mathbf y \in \left\lbrace 0, 1 \right\rbrace^L$
        we can write

        \begin{equation}
            \text{P}@k := \frac{1}{k} \sum_{l\in \text{rank}_k (\hat{\mathbf y})} \mathbf y_l
        \end{equation}

        where $\text{rank}_k(\mathbf y)$ returns the $k$ largest indices of $y$ ranked in descending order.

        We have decided not to focus on the training time of the evaluated indices. This
        descision is motivated by the fact, that in most practical use-cases, where
        approximate inference methods will be used, the training time is a one-time cost that
        often can be amortized (e.g by using more compute-power).
        Inference in these cases, however, is the time-sensitive component
        that cannot be easily sped-up (e.g the inference happens on a mobile device, or a small web-server).

    \subsubsection{Datasets}\label{sec:datasets}

        We have used four datasets to evaluate performance of our algorithms.

        % TODO(elanmart): get proper references from http://corpus-texmex.irisa.fr/
        Two of them, \texttt{SIFT} and \texttt{SIFTsmall} were introduced in $[ref]$ and their
        main purpose was to allow for evaluation of (approximate) \texttt{knn} algorithms.

        For these datasets a set of $n$ "databse" vectors is given with dimensionality
        $d=128$, as well as a set of "query" vectors $\in \cal{R}^d$
        and the "groundtruth" indices. For every "query" vector there are 100 "groundtruth" indices
        that point to the closest (according to L2 distance) vectors in the database.

        First of all, for every query, we consider only the first vector in the groundtruth as relevant.
        Secondly, we recompute the groundtruth indices so that they're sorted accroding to inner-product
        metric.



    \subsubsection{Parameter grid}


    \subsubsection{Baseline (IVF)}


    \subsubsection{Amazon}

\subsection{Analysis of results}

The amount of data collected is quite large, so directly plotting the results would be
unreadable. Instead, we processed the data selecting, for each algorithm and dataset,
the nondominated parameter choices --- that is those, for which there is no other
parameter choice that gives results that are better in both query time and precision.

Algorithms were always called with request for top 100 vectors, but we recorded
precision at a couple of points: 1, 5, 25 and 100. It turns out the precision vs time
plots are very similar for each of those values --- to save space, we only show
graphs for precision at 100.

% TODO trochę mały font w wykresach

\begin{figure}[H]
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/Amazon-3M-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/sift-p100.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/WikiLSHTC-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/siftsmall-p100.eps}
	}
\caption{Pareto frontiers for each algorithm.}
\end{figure}

The graphs show some similarity --- in all cases $K$-Means and IVF algorithms were much better
than the alternatives. ALSH algorithm gave very poor results on Amazon-3M and Wiki-LSHTC
datasets --- apparently sift vectors have some regularity that ALSH could exploit better.
Still, the precision did not exceed approximately $0.60$ for reasonable speedups even on
this dataset. Finally, the quantization-based algorithm, although it was able to give
reasonable precision, it was so slow, that the only dataset it did not time
out\footnote{
    Timeout was set to twice the time of brute force search.
}
was siftsmall dataset. It contained only ten thousand vectors though and was included
just for completion.

IVF and $K$-Means curves follow each other quite closely --- it could be expected though,
since $K$-Means algorithm is generalization of IVF. In almost all places $K$-Means are even
better than IVF --- again, this can be explained by having more parameters to be tested.
To check whether this is caused by possibly faster implementation or the introduction of
hierarchy, we split $K$-Means curve into three lines, each corresponding to a single
number of layers (for example, KMeans-2 has two layers).

% TODO jak wyżej - zwiększyć font na wykresach
\begin{figure}[H]
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/Amazon-3M-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/sift-p100.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/WikiLSHTC-p100.eps}
	}
	\caption{Pareto frontiers for K-Means (split) and IVF.}
\end{figure}

From these graphs, we can see that in general IVF is approximately $30\% - 50\%$ faster
(on horizontal axis) than $K$-Means when using just a single layer. Both algorithms are
equivalent in this case, so this means Faiss library is more optimized for this parameter
choice\footnote{
    Checking its source we can see it performs inner products of each query with all centroids
    (from the first and only layer) all at once, before continuing to answer each query
    using its candidates. This allows one to use heavily optimized matrix-matrix multiplication
    routines for the first phase, with additional memory locality advantages.
    In principle, this optimization
    could be implemen`ted for hierarchical $K$-Means as well (for the first layer only).
}.
In contrast, $K$-Means are much better when allowed to use two or three layers. Addition
of hierarchy speeds calculation to the level of IVF --- for some parameters even surpassing
it, especially for Amazon-3M dataset.

The difference between two-layer $K$-Means and three-layer ones is not as amplified. Since
training time (not pictured here) usually rises quite significantly with number of layers,
the minor additional speedup may not be worth it in practice. Four and more layers
would probably not have noticable impact either.

It is also notable that two- and three-layer $K$-Means curves continue much further left
than IVF. Of course, such quick results come with significant impact on accuracy, but
IVF cannot even reach this speed. In a sense, its accuracy is zero there.
A possible explanation for this cutoff comes from its complexity analysis as described
in introduction to $K$-Means algorithm chapter. No matter what precision is, the minimal
complexity of flat $K$-Means (equivalent to IVF) is $ O \left( \sqrt{N} \right )$.
Two- or three-layer $K$-Means bring this limit down to $ O \left( \sqrt[3]{N} \right) $,
or $ O \left( \sqrt[4]{N} \right) $ respectively\footnote{
    Admittedly with a small multiplicative slowdown.
}.

TODO: table with nondominated parameters, additional analysis
\newpage
