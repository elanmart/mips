In this work we implemented and benchmarked three different MIPS 
approximation algorithms
against Faiss-provided optimized full search and IVF algorithm.
From our tests, we see that quantization-based and ALSH algorithms
are not performing too well --- at least on datasets we chose.
Quantization algorithm is quite predictable in speed --- it performed
comparably, or even worse than full search on all large benchmarks.
On the other hand, ALSH gives results dependent on dataset --- for
two of three datasets we used, its precision was too low to be useful\footnote{
Except for parameter choices which gave runtime slower than full search,
which were not pictured on graphs. These results are useless too, of course.}.
Further research might be needed to find dataset features causing
ALSH to perform well.

The clear winner here is Hierarchical $K$-Means algorithm. It was consistently
giving better results than other algorithms, including Faiss' IVF. It can be also
highly parametrized, allowing wide range of points on time-precision tradeoff,
including speedups a couple times higher than the best we could get from IVF.

Auvolat $K$-Means paper suggests cluster sizes being powers of $(L+1)$-th
root of number of vectors, $L$ being number of intermediate layers. Our
small contribution to this algorithm was calculation of speed-optimal
cluster sizes. From our tests, this improvement helps achieve higher precision
earlier\footnote{
See graphs of precision vs. time for various $n_{train}$. Auvolat version
corresponds to curve $n_{train} = 1$, which other versions exceed for larger
precision and time values.
}.

In our benchmarks, we used only integral values of $n_{train}$, which set
a natural lower bound of this parameter to 1. There is no reason why this
parameter, determining tree shape, could not be set to even lower real values.
Extrapolating from our tests, we would expect to achieve even higher speedups
(probably with some small precision loss).

[TODO] Weź ktoś napisz o tym że są fajne zastosowania, @marcin, że można w
pythonie (łatwo!) przyspieszać i że jesteśmy fajni...
